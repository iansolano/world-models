{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input dataset\n",
    "Dataset used is celebA : https://www.kaggle.com/jessicali9530/celeba-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_size = 20 * 1000\n",
    "batch_size = 32\n",
    "\n",
    "def ds_gen():\n",
    "    dirname = '../results/WorldModels/CarRacing-v0/record'\n",
    "    filenames = os.listdir(dirname)[:10000] # only use first 10k episodes\n",
    "    n = len(filenames)\n",
    "    for j, fname in enumerate(filenames):\n",
    "        if not fname.endswith('npz'): \n",
    "            continue\n",
    "        file_path = os.path.join(dirname, fname)\n",
    "        with np.load(file_path) as data:\n",
    "            N = data['obs'].shape[0]\n",
    "            for i, img in enumerate(data['obs']):\n",
    "                img_i = img / 255.0\n",
    "                yield img_i\n",
    "\n",
    "batch_gen = tf.data.Dataset.from_generator(ds_gen, output_types=tf.float32, output_shapes=(64, 64, 3))\n",
    "batch_gen = batch_gen.shuffle(shuffle_size, reshuffle_each_iteration=True).batch(batch_size)\n",
    "batch_gen = batch_gen.prefetch(100) # prefetch 100 batches in the buffer #tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_DIM = 64\n",
    "DEPTH = 32\n",
    "LATENT_DEPTH = 512\n",
    "K_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    mean, logsigma = args\n",
    "    epsilon = keras.backend.random_normal(shape=keras.backend.shape(mean))\n",
    "    return mean + tf.exp(logsigma / 2) * epsilon\n",
    "\n",
    "def encoder():\n",
    "    input_E = keras.layers.Input(shape=(IM_DIM, IM_DIM, 3))\n",
    "    \n",
    "    X = keras.layers.Conv2D(filters=DEPTH*2, kernel_size=K_SIZE, strides=2, padding='same')(input_E)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(filters=DEPTH*4, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(filters=DEPTH*8, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Flatten()(X)\n",
    "    X = keras.layers.Dense(LATENT_DEPTH)(X)    \n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    mean = keras.layers.Dense(LATENT_DEPTH,activation=\"tanh\")(X)\n",
    "    logsigma = keras.layers.Dense(LATENT_DEPTH,activation=\"tanh\")(X)\n",
    "    latent = keras.layers.Lambda(sampling, output_shape=(LATENT_DEPTH,))([mean, logsigma])\n",
    "    \n",
    "    kl_loss = 1 + logsigma - keras.backend.square(mean) - keras.backend.exp(logsigma)\n",
    "    kl_loss = keras.backend.mean(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    \n",
    "    return keras.models.Model(input_E, [latent,kl_loss])\n",
    "\n",
    "def generator():\n",
    "    input_G = keras.layers.Input(shape=(LATENT_DEPTH,))\n",
    "\n",
    "    X = keras.layers.Dense(8*8*DEPTH*8)(input_G)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    X = keras.layers.Reshape((8, 8, DEPTH * 8))(X)\n",
    "    \n",
    "    X = keras.layers.Conv2DTranspose(filters=DEPTH*8, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "\n",
    "    X = keras.layers.Conv2DTranspose(filters=DEPTH*4, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Conv2DTranspose(filters=DEPTH, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Conv2D(filters=3, kernel_size=K_SIZE, padding='same')(X)\n",
    "    X = keras.layers.Activation('sigmoid')(X)\n",
    "\n",
    "    return keras.models.Model(input_G, X)\n",
    "\n",
    "def discriminator():\n",
    "    input_D = keras.layers.Input(shape=(IM_DIM, IM_DIM, 3))\n",
    "    \n",
    "    X = keras.layers.Conv2D(filters=DEPTH, kernel_size=K_SIZE, strides=2, padding='same')(input_D)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Conv2D(filters=DEPTH*4, kernel_size=K_SIZE, strides=2, padding='same')(input_D)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(filters=DEPTH*8, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(filters=DEPTH*8, kernel_size=K_SIZE, padding='same')(X)\n",
    "    inner_output = keras.layers.Flatten()(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Flatten()(X)\n",
    "    X = keras.layers.Dense(DEPTH*8)(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    output = keras.layers.Dense(1)(X)    \n",
    "    \n",
    "    return keras.models.Model(input_D, [output, inner_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in im_dataset_batch.take(1) :\n",
    "for x in batch_gen.take(1) :\n",
    "    test_images = x\n",
    "test_r = tf.random.normal((batch_size, LATENT_DEPTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = encoder()\n",
    "G = generator()\n",
    "D = discriminator() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step : 10  gan_loss 0.24 vae_loss 0.166 fake_dis_loss 0.252 r_dis_loss 0.252 t_dis_loss 0.229 vae_inner_loss 0.001 E_loss 0.011 D_loss 0.24 kl_loss 0.001 normal_loss 0.095"
     ]
    }
   ],
   "source": [
    "lr=0.0001\n",
    "#lr=0.0001\n",
    "E_opt = keras.optimizers.Adam(lr=lr)\n",
    "G_opt = keras.optimizers.Adam(lr=lr)\n",
    "D_opt = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "inner_loss_coef = 1\n",
    "normal_coef = 0.1\n",
    "kl_coef = 0.01\n",
    "\n",
    "@tf.function\n",
    "def train_step_vaegan(x):\n",
    "    lattent_r =  tf.random.normal((batch_size, LATENT_DEPTH))\n",
    "    with tf.GradientTape(persistent=True) as tape :\n",
    "        lattent,kl_loss = E(x)\n",
    "        fake = G(lattent)\n",
    "        dis_fake,dis_inner_fake = D(fake)\n",
    "        dis_fake_r,_ = D(G(lattent_r))\n",
    "        dis_true,dis_inner_true = D(x)\n",
    "\n",
    "        vae_inner = dis_inner_fake-dis_inner_true\n",
    "        vae_inner = vae_inner*vae_inner\n",
    "        \n",
    "        mean,var = tf.nn.moments(E(x)[0], axes=0)\n",
    "        var_to_one = var - 1\n",
    "        \n",
    "        normal_loss = tf.reduce_mean(mean*mean) + tf.reduce_mean(var_to_one*var_to_one)\n",
    "        \n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        vae_diff_loss = tf.reduce_mean(vae_inner)\n",
    "        f_dis_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(tf.zeros_like(dis_fake), dis_fake))\n",
    "        r_dis_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(tf.zeros_like(dis_fake_r), dis_fake_r))\n",
    "        t_dis_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(tf.ones_like(dis_true), dis_true))\n",
    "        gan_loss = (0.5*t_dis_loss + 0.25*f_dis_loss + 0.25*r_dis_loss)\n",
    "        vae_loss = tf.reduce_mean(tf.abs(x-fake)) \n",
    "        E_loss = vae_diff_loss + kl_coef*kl_loss + normal_coef*normal_loss\n",
    "        G_loss = inner_loss_coef*vae_diff_loss - gan_loss\n",
    "        D_loss = gan_loss\n",
    "    \n",
    "    E_grad = tape.gradient(E_loss,E.trainable_variables)\n",
    "    G_grad = tape.gradient(G_loss,G.trainable_variables)\n",
    "    D_grad = tape.gradient(D_loss,D.trainable_variables)\n",
    "    del tape\n",
    "    E_opt.apply_gradients(zip(E_grad, E.trainable_variables))\n",
    "    G_opt.apply_gradients(zip(G_grad, G.trainable_variables))\n",
    "    D_opt.apply_gradients(zip(D_grad, D.trainable_variables))\n",
    "\n",
    "    return [gan_loss, vae_loss, f_dis_loss, r_dis_loss, t_dis_loss, vae_diff_loss, E_loss, D_loss, kl_loss, normal_loss]\n",
    "\n",
    "step = 0\n",
    "max_step = 10000000\n",
    "log_freq,img_log_freq = 10, 100\n",
    "save_freq,save_number_mult = 1000, 10000\n",
    "\n",
    "metrics_names = [\"gan_loss\", \"vae_loss\", \"fake_dis_loss\", \"r_dis_loss\", \"t_dis_loss\", \"vae_inner_loss\", \"E_loss\", \"D_loss\", \"kl_loss\", \"normal_loss\"]\n",
    "metrics = []\n",
    "for m in metrics_names :\n",
    "    metrics.append(tf.keras.metrics.Mean('m', dtype=tf.float32))\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = ('logs/sep_D%dL%d/' % (DEPTH,LATENT_DEPTH)) + current_time \n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "name = ('sep_D%dL%d' % (DEPTH,LATENT_DEPTH))\n",
    "\n",
    "def save_model() :\n",
    "    nb = str(step // save_number_mult)\n",
    "    D.save('saved-models/D_training_' + nb + '.h5')\n",
    "    G.save('saved-models/G_training_' + nb + '.h5')\n",
    "    E.save('saved-models/E_training_' + nb + '.h5')\n",
    "\n",
    "def print_metrics() :\n",
    "    s = \"\"\n",
    "    for name,metric in zip(metrics_names,metrics) :\n",
    "        s+= \" \" + name + \" \" + str(np.around(metric.result().numpy(), 3)) \n",
    "    print(f\"\\rStep : \" + str(step) + \" \" + s, end=\"\", flush=True)\n",
    "    with train_summary_writer.as_default():\n",
    "        for name,metric in zip(metrics_names,metrics) :\n",
    "            tf.summary.scalar(name, metric.result(), step=step)\n",
    "    for metric in metrics : \n",
    "        metric.reset_states()\n",
    "\n",
    "        \n",
    "def log_images() :\n",
    "    with train_summary_writer.as_default():\n",
    "        lattent,_ = E(test_images)\n",
    "        fake = G(lattent)\n",
    "        fake_r = G(test_r)\n",
    "        tf.summary.image(\"reconstructed image\", fake[:8], step=step, max_outputs=8)\n",
    "        tf.summary.image(\"random image\", fake_r[:8], step=step, max_outputs=8)\n",
    "        dis_fake,inner_dis_fake = D(fake)\n",
    "        dis_fake_r,inner_dis_fake_r = D(fake_r)\n",
    "        dis_true,inner_dis_true = D(test_images)\n",
    "        tf.summary.histogram(\"dis fake\", inner_dis_fake, step=step, buckets=20)\n",
    "        tf.summary.histogram(\"dis true\", inner_dis_true, step=step, buckets=20)\n",
    "        tf.summary.histogram(\"dis random\", inner_dis_fake_r, step=step, buckets=20)\n",
    "        tf.summary.histogram(\"dis lattent\", lattent, step=step, buckets=20)\n",
    "        tf.summary.histogram(\"dis normal\", tf.random.normal((batch_size, LATENT_DEPTH)), step=step, buckets=20)        \n",
    "\n",
    "for x in batch_gen :\n",
    "    step += 1\n",
    "    if not step % log_freq :\n",
    "        print_metrics()\n",
    "    if not step % img_log_freq :\n",
    "        log_images()\n",
    "    if not step % save_freq :\n",
    "        save_model()\n",
    "    \n",
    "    results = train_step_vaegan(x)\n",
    "    for metric,result in zip(metrics, results) :\n",
    "        metric(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "ab00fc820ae433bc0f6038de8d9aab1d652949c05581d2eb9e013f132e67bf09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
